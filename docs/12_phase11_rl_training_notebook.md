# **12_phase11_rl_training_notebook.md â€” TrÃ©nink RL agenta v Jupyter Notebooku**

## ğŸ¯ 1. CÃ­le fÃ¡ze 11 (RL trÃ©nink)

V tÃ©to fÃ¡zi pÅ™eneseme myÅ¡lenku **Q-learning AI** z pÅ™edchozÃ­ fÃ¡ze do samostatnÃ©ho **trÃ©ninkovÃ©ho prostÅ™edÃ­** v Jupyter Notebooku.

NauÄÃ­Å¡ se:

* vytvoÅ™it jednoduchÃ© â€simulaÄnÃ­â€œ prostÅ™edÃ­ MULTIPONGU pro RL (zjednoduÅ¡enÃ¡ verze hry)
* napsat trÃ©ninkovou smyÄku RL agenta (Q-learning)
* sbÃ­rat odmÄ›ny a aktualizovat Q-tabuli
* uklÃ¡dat nauÄenÃ½ model na disk (napÅ™. `q_table.pkl`)
* naÄÃ­st model zpÄ›t ve hÅ™e MULTIPONG jako AI hrÃ¡Äe

Tahle fÃ¡ze je ideÃ¡lnÃ­ pro projektovÃ© prÃ¡ce a seminÃ¡Å™e â€“ nenÃ­ nutnÃ¡ pro zÃ¡kladnÃ­ funkÄnost hry, ale je velmi inspirativnÃ­.

---

## ğŸ§  2. ProÄ trÃ©novat v Jupyter Notebooku?

Jupyter Notebook je vhodnÃ½ pro:

* **experimentovÃ¡nÃ­** â€“ mÄ›nit parametry uÄenÃ­ a hned vidÄ›t vÃ½sledky
* **vizualizaci** â€“ grafy prÅ¯bÄ›hu odmÄ›ny, konvergence, atd.
* **komentovanÃ½ kÃ³d** â€“ vysvÄ›tlenÃ­ krok za krokem
* **spoluprÃ¡ci s Copilotem** â€“ krÃ¡tkÃ© cell-ovÃ© prompty, rychlÃ© Ãºpravy

Hru MULTIPONG nechÃ¡me bÄ›Å¾et jako plnohodnotnou sÃ­Å¥ovou aplikaci, ale RL trÃ©nink si zjednoduÅ¡Ã­me do â€simulaÄnÃ­ho modeluâ€œ, kterÃ½ bÄ›Å¾Ã­ ÄistÄ› v Pythonu.

---

## ğŸ“ 3. Struktura projektu s notebookem

DoplnÃ­me sloÅ¾ku `notebooks/`:

```
multipong/
â”‚
â”œâ”€â”€ notebooks/
â”‚     â”œâ”€â”€ rl_training_multpong.ipynb
â”‚
â”œâ”€â”€ multipong/
â”‚     â”œâ”€â”€ ai/
â”‚     â”‚     â”œâ”€â”€ qlearning_ai.py
â”‚     â”‚     â”œâ”€â”€ ...
â”‚     â””â”€â”€ engine/
â”‚
â””â”€â”€ docs/
      â””â”€â”€ 12_phase11_rl_training_notebook.md
```

---

## ğŸ§± 4. ZjednoduÅ¡enÃ© â€RL prostÅ™edÃ­â€œ pro Pong

Pro RL nepotÅ™ebujeme celou komplexitu MULTIPONGU:

* nepotÅ™ebujeme tÃ½my
* nepotÅ™ebujeme vÃ­ce pÃ¡lek
* nepotÅ™ebujeme branky â†’ staÄÃ­ â€odrÃ¡Å¾enÃ­â€œ a sledovÃ¡nÃ­, jestli agent trefil mÃ­Äek

VytvoÅ™Ã­me **minimalistickÃ© prostÅ™edÃ­**:

* jeden mÃ­Äek
* jedna pÃ¡lka (AI)
* odraz od hornÃ­ a dolnÃ­ stÄ›ny
* epizoda skonÄÃ­, kdyÅ¾ mÃ­Äek proletÃ­ za pÃ¡lkou

### 4.1 TÅ™Ã­da `RLPongEnv` (v Python modulu)

DoporuÄenÃ©: vytvoÅ™it pomocnÃ½ modul (napÅ™. `multipong/ai/rl_env.py`), kterÃ½ mÅ¯Å¾eme importovat v notebooku.

```python
# multipong/ai/rl_env.py

from dataclasses import dataclass

@dataclass
class State:
    ball_x: float
    ball_y: float
    ball_vx: float
    ball_vy: float
    paddle_y: float


class RLPongEnv:
    """
    ZjednoduÅ¡enÃ© RL prostÅ™edÃ­:
    - jedna pÃ¡lka (vlevo)
    - mÃ­Äek se pohybuje doprava/doleva
    - odraz od hornÃ­/dolnÃ­ stÄ›ny
    - epizoda konÄÃ­, kdyÅ¾ mÃ­Äek proletÃ­ vlevo za pÃ¡lkou
    """

    def __init__(self, width=400, height=300,
                 paddle_height=60, paddle_speed=5, ball_speed=4):
        self.width = width
        self.height = height
        self.paddle_height = paddle_height
        self.paddle_speed = paddle_speed
        self.ball_speed = ball_speed

        self.reset()

    def reset(self):
        self.ball_x = self.width // 2
        self.ball_y = self.height // 2
        self.ball_vx = self.ball_speed
        self.ball_vy = self.ball_speed
        self.paddle_y = self.height // 2 - self.paddle_height // 2

        return self._get_state()

    def _get_state(self):
        return State(
            ball_x=self.ball_x,
            ball_y=self.ball_y,
            ball_vx=self.ball_vx,
            ball_vy=self.ball_vy,
            paddle_y=self.paddle_y
        )

    def step(self, action):
        """
        action: 0 = nic, 1 = nahoru, 2 = dolÅ¯
        vracÃ­: next_state, reward, done
        """

        # pohyb pÃ¡lky
        if action == 1:
            self.paddle_y -= self.paddle_speed
        elif action == 2:
            self.paddle_y += self.paddle_speed

        self.paddle_y = max(0, min(self.height - self.paddle_height, self.paddle_y))

        # pohyb mÃ­Äku
        self.ball_x += self.ball_vx
        self.ball_y += self.ball_vy

        # odraz od stÄ›n
        if self.ball_y <= 0 or self.ball_y >= self.height:
            self.ball_vy *= -1

        reward = 0
        done = False

        # kolize s pÃ¡lkou (pÃ¡lka je vlevo)
        if self.ball_x <= 20:  # x pozice pÃ¡lky
            if self.paddle_y <= self.ball_y <= self.paddle_y + self.paddle_height:
                # zÃ¡sah mÃ­Äku
                self.ball_vx *= -1
                reward = 1    # odmÄ›na za zÃ¡sah
            else:
                # netrefil â†’ konec epizody
                reward = -5
                done = True

        # mÃ­Äek vpravo â€“ jen odraz (aby se vracel)
        if self.ball_x >= self.width:
            self.ball_vx *= -1

        return self._get_state(), reward, done
```

---

## ğŸ““ 5. Jupyter Notebook â€“ zÃ¡kladnÃ­ kostra

Notebook `rl_training_multpong.ipynb` mÅ¯Å¾e mÃ­t tyto sekce:

1. Importy a pÅ™Ã­prava prostÅ™edÃ­
2. Diskretizace stavu
3. Q-learning smyÄka
4. Vizualizace prÅ¯bÄ›hu odmÄ›ny
5. UloÅ¾enÃ­ nauÄenÃ© Q-tabule
6. KrÃ¡tkÃ¡ zÃ¡vÄ›reÄnÃ¡ evaluace

NÃ­Å¾e je obsah, kterÃ½ do notebooku postupnÄ› vloÅ¾Ã­Å¡.

---

### 5.1 Importy

```python
import numpy as np
import random
import pickle
from multipong.ai.rl_env import RLPongEnv, State
```

---

### 5.2 Diskretizace stavu

Q-learning pracuje s **diskrÃ©tnÃ­m stavovÃ½m prostorem**.
ZjednoduÅ¡Ã­me:

* rozsekÃ¡me vÃ½Å¡ku obrazovky na nÄ›kolik â€zÃ³nâ€œ
* sledujeme relativnÃ­ vertikÃ¡lnÃ­ polohu mÃ­Äku vÅ¯Äi pÃ¡lce
* sledujeme smÄ›r pohybu mÃ­Äku (nahoru/dolÅ¯)

```python
def encode_state(state: State, env: RLPongEnv,
                 num_bins=10):

    # relativnÃ­ pozice mÃ­Äku vÅ¯Äi pÃ¡lce
    rel_y = state.ball_y - state.paddle_y
    rel_y_norm = rel_y / env.height  # 0â€“1
    rel_bin = int(rel_y_norm * num_bins)
    rel_bin = max(0, min(num_bins-1, rel_bin))

    # smÄ›r pohybu mÃ­Äku
    dir_y = 0 if state.ball_vy == 0 else (1 if state.ball_vy > 0 else -1)

    return (rel_bin, dir_y)
```

Akce:

```python
ACTIONS = [0, 1, 2]  # stay, up, down
```

---

### 5.3 Inicializace Q-tabule

```python
Q = {}  # Q[(state)] = np.array([Q_a0, Q_a1, Q_a2])

def get_Q(state_key):
    if state_key not in Q:
        Q[state_key] = np.zeros(len(ACTIONS))
    return Q[state_key]
```

---

### 5.4 TrÃ©ninkovÃ¡ smyÄka Q-learningu

Hyperparametry:

```python
episodes = 5000
alpha = 0.1     # learning rate
gamma = 0.95    # discount factor
epsilon = 0.1   # epsilon-greedy
env = RLPongEnv()
```

TrÃ©nink:

```python
rewards_per_episode = []

for ep in range(episodes):
    state = env.reset()
    state_key = encode_state(state, env)
    total_reward = 0

    done = False
    while not done:
        # epsilon-greedy vÃ½bÄ›r akce
        if random.random() < epsilon:
            action_idx = random.randint(0, len(ACTIONS)-1)
        else:
            q_vals = get_Q(state_key)
            action_idx = int(np.argmax(q_vals))

        action = ACTIONS[action_idx]
        next_state, reward, done = env.step(action)
        total_reward += reward

        next_state_key = encode_state(next_state, env)

        # aktualizace Q
        q_vals = get_Q(state_key)
        q_next = get_Q(next_state_key)
        q_vals[action_idx] += alpha * (reward + gamma * np.max(q_next) - q_vals[action_idx])

        state_key = next_state_key

    rewards_per_episode.append(total_reward)

    if (ep+1) % 500 == 0:
        print(f"Epizoda {ep+1}/{episodes}, prÅ¯mÄ›rnÃ¡ odmÄ›na poslednÃ­ch 100: {np.mean(rewards_per_episode[-100:]):.2f}")
```

---

### 5.5 Vizualizace vÃ½voje odmÄ›ny

Pokud mÃ¡Å¡ k dispozici matplotlib:

```python
import matplotlib.pyplot as plt

window = 100
smoothed = [np.mean(rewards_per_episode[max(0, i-window):i+1]) for i in range(len(rewards_per_episode))]

plt.plot(smoothed)
plt.xlabel("Epizoda")
plt.ylabel("PrÅ¯mÄ›rnÃ¡ odmÄ›na (klouzavÃ½ prÅ¯mÄ›r)")
plt.title("UÄenÃ­ RL agenta v RLPongEnv")
plt.show()
```

---

### 5.6 UloÅ¾enÃ­ nauÄenÃ© Q-tabule

```python
with open("q_table_multipong.pkl", "wb") as f:
    pickle.dump(Q, f)
```

Soubor `q_table_multipong.pkl` pak zkopÃ­rujeÅ¡ do napÅ™.:

```
multipong/ai/models/q_table_multipong.pkl
```

---

## ğŸ”„ 6. NaÄtenÃ­ modelu ve hÅ™e MULTIPONG

V `QLearningAI` tÅ™Ã­dÄ› upravÃ­Å¡:

```python
import pickle
from pathlib import Path

class QLearningAI(BaseAI):
    def __init__(self, table_path="multipong/ai/models/q_table_multipong.pkl",
                 epsilon=0.0):  # 0 = Å¾Ã¡dnÃ¡ explorace
        self.epsilon = epsilon
        self.Q = {}
        table_file = Path(table_path)
        if table_file.exists():
            with open(table_file, "rb") as f:
                self.Q = pickle.load(f)
```

A zÃ¡roveÅˆ pÅ™izpÅ¯sobÃ­Å¡ `encode_state()` v AI tak, aby pouÅ¾Ã­valo **stejnou logiku jako v notebooku** (stejnÃ© binovÃ¡nÃ­, stejnÃ¡ struktura klÃ­Äe).

TÃ­m zÃ­skÃ¡Å¡:

* **trÃ©nink RL agenta offline v notebooku**
* **pouÅ¾itÃ­ nauÄenÃ©ho chovÃ¡nÃ­ v reÃ¡lnÃ© MULTIPONG hÅ™e**

---

## ğŸ§ª 7. Mini Ãºkoly pro studenty

1. **ZmÄ›Åˆ odmÄ›novou funkci**
   Zkus +2 za zÃ¡sah, +10 za 5 zÃ¡sahÅ¯ za sebou, âˆ’10 za netrefenÃ­.

2. **Porovnej rÅ¯znÃ© hyperparametry**
   VyzkouÅ¡ej rÅ¯znÃ© kombinace Î±, Î³, Îµ a zakresli, jak se mÄ›nÃ­ kÅ™ivka uÄenÃ­.

3. **RozÅ¡Ã­Å™ stav**
   PÅ™idej informaci o horizontÃ¡lnÃ­ pozici mÃ­Äku (blÃ­zko / daleko).

4. **Copilot prompt**

   > â€Analyzuj tento Q-learning trÃ©nink a navrhni moÅ¾nosti, jak zabrÃ¡nit pÅ™euÄenÃ­ (overfittingu) i v tomto jednoduchÃ©m prostÅ™edÃ­.â€œ

---

## ğŸ“˜ 8. ShrnutÃ­ fÃ¡ze

V tÃ©to fÃ¡zi jsme:

* vytvoÅ™ili zjednoduÅ¡enÃ© RL prostÅ™edÃ­ pro Pong
* implementovali Q-learning v Jupyter Notebooku
* nauÄili agenta chovat se optimÃ¡lnÄ›ji neÅ¾ nÃ¡hodnÄ›
* uloÅ¾ili nauÄenÃ½ model
* pÅ™ipravili jeho integraci do reÃ¡lnÃ© hry jako AI hrÃ¡Äe

Tato fÃ¡ze uÅ¾ pÅ™ekraÄuje bÄ›Å¾nou stÅ™edoÅ¡kolskou ÃºroveÅˆ, ale prÃ¡vÄ› proto je skvÄ›lÃ¡ pro talentovanÃ© studenty a projektovÃ© prÃ¡ce.


