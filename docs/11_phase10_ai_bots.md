# **11_phase10_ai_bots.md â€” UmÄ›lÃ¡ inteligence pro MULTIPONG (AI Bots)**

## ğŸ¯ 1. CÃ­le fÃ¡ze 10

Ve fÃ¡zi 10 vytvoÅ™Ã­me architekturu a implementace AI hrÃ¡ÄÅ¯, kteÅ™Ã­ mohou:

* doplnit tÃ½m, kdyÅ¾ nenÃ­ dost lidskÃ½ch hrÃ¡ÄÅ¯
* hrÃ¡t proti studentovi (PvE)
* slouÅ¾it jako demonstraÄnÃ­ modely pro vÃ½uku
* bÃ½t trÃ©novÃ¡ni pomocÃ­ jednoduchÃ©ho RL algoritmu
* bÃ½t pozdÄ›ji nahrazeni neurÃ¡lnÃ­ sÃ­tÃ­

CÃ­lem je ukÃ¡zat rÅ¯znÃ© **ÃºrovnÄ› obtÃ­Å¾nosti AI**, od nejjednoduÅ¡Å¡Ã­ aÅ¾ po adaptivnÃ­.

---

# ğŸ§  2. Typy AI hrÃ¡ÄÅ¯

Navrhujeme 4 ÃºrovnÄ› AI:

1. **LEVEL 0 â€“ StatickÃ¡ AI**
   PÃ¡lka se nehÃ½be, vhodnÃ© pro debugging.

2. **LEVEL 1 â€“ HeuristickÃ¡ AI**
   Sleduje pozici mÃ­Äku a snaÅ¾Ã­ se drÅ¾et â€stÅ™edâ€œ.

3. **LEVEL 2 â€“ PrediktivnÃ­ AI**
   Odhaduje dopad mÃ­Äku podle rychlosti a Ãºhlu.

4. **LEVEL 3 â€“ RL Agent (Q-learning)**
   UÄÃ­ se odmÄ›ÅˆovÃ¡nÃ­m:

   * +1 za zÃ¡sah
   * +5 za gÃ³l
   * âˆ’2 za obdrÅ¾enÃ½ gÃ³l

VolitelnÃ©:

5. **LEVEL 4 â€“ NeuronovÃ¡ sÃ­Å¥ (TensorFlow)**
   TrÃ©novanÃ¡ na zÃ¡znamech zÃ¡pasÅ¯.

---

# ğŸ“ 3. Struktura AI modulu

VytvoÅ™Ã­me novou sloÅ¾ku:

```
multipong/
â”‚
â”œâ”€â”€ multipong/
â”‚     â”œâ”€â”€ ai/
â”‚     â”‚     â”œâ”€â”€ base_ai.py
â”‚     â”‚     â”œâ”€â”€ simple_ai.py
â”‚     â”‚     â”œâ”€â”€ predictive_ai.py
â”‚     â”‚     â”œâ”€â”€ qlearning_ai.py
â”‚     â”‚     â”œâ”€â”€ nn_ai.py   (volitelnÃ©)
â”‚     â”‚     â””â”€â”€ utils.py
â”‚     â””â”€â”€ engine/
â”‚
â””â”€â”€ docs/
      â””â”€â”€ 11_phase10_ai_bots.md
```

---

# ğŸŸ¦ 4. AbstraktnÃ­ tÅ™Ã­da AI â€“ `base_ai.py`

Toto je zÃ¡klad pro vÅ¡echny AI typy.

```python
class BaseAI:
    """
    AbstraktnÃ­ AI hrÃ¡Ä â€“ poskytuje metodu 'decide'.
    KaÅ¾dÃ¡ AI vracÃ­ dict: {"up": bool, "down": bool}
    """

    def decide(self, paddle, ball, arena):
        raise NotImplementedError
```

---

# ğŸŸ© 5. LEVEL 1 â€“ JednoduchÃ¡ heuristickÃ¡ AI

### Princip:

* pokud je mÃ­Äek vÃ½Å¡e neÅ¾ pÃ¡lka â†’ jdi nahoru
* pokud je nÃ­Å¾e â†’ jdi dolÅ¯

`soubor: ai/simple_ai.py`

```python
from .base_ai import BaseAI

class SimpleAI(BaseAI):
    def __init__(self, reaction_speed=1.0):
        self.reaction_speed = reaction_speed

    def decide(self, paddle, ball, arena):
        # jednoduchÃ© sledovÃ¡nÃ­ mÃ­Äku
        target_y = ball.y
        center = paddle.y + paddle.height / 2

        up = down = False

        if center > target_y:
            up = True
        elif center < target_y:
            down = True

        return {"up": up, "down": down}
```

---

# ğŸŸ§ 6. LEVEL 2 â€“ PrediktivnÃ­ AI

Predikce mÃ­sta dopadu mÃ­Äku:

* lineÃ¡rnÃ­ extrapolace trajektorie
* odraz od hornÃ­/dolnÃ­ stÄ›ny
* AI se snaÅ¾Ã­ bÃ½t o krok napÅ™ed

`soubor: ai/predictive_ai.py`

```python
from .base_ai import BaseAI
import copy

class PredictiveAI(BaseAI):
    def __init__(self, prediction_steps=200):
        self.prediction_steps = prediction_steps

    def decide(self, paddle, ball, arena):
        # simuluj budoucÃ­ pohyb mÃ­Äku
        sim = copy.copy(ball)

        for _ in range(self.prediction_steps):
            sim.update()

        target_y = sim.y
        center = paddle.y + paddle.height / 2

        return {
            "up": center > target_y,
            "down": center < target_y
        }
```

### PoznÃ¡mka pro studenty:

* lze experimentovat s poÄtem predikÄnÃ­ch krokÅ¯
* lze pÅ™idat Å¡um (chyby) pro realistiÄtÄ›jÅ¡Ã­ chovÃ¡nÃ­

---

# ğŸŸ¥ 7. LEVEL 3 â€“ RL Agent (Q-learning)

Toto je vÃ½ukovÃ¡ ukÃ¡zka reinforcement learningu v jednoduchÃ© formÄ›.

### Stav Q-learningu mÅ¯Å¾e obsahovat:

* relativnÃ­ pozici mÃ­Äku (`ball.y - paddle.y`)
* smÄ›r mÃ­Äku (`sign(ball.vy)`)
* rychlost mÃ­Äku (`abs(ball.vx)`)

Stav zakÃ³dujeme jako tuple:

```
state = (ball_zone, ball_direction, speed_bucket)
```

### Akce:

* 0: nic
* 1: nahoru
* 2: dolÅ¯

### OdmÄ›ny:

* +1 za zÃ¡sah
* +5 za gÃ³l
* â€“3 za obdrÅ¾enÃ½ gÃ³l

`soubor: ai/qlearning_ai.py`

```python
import random
from .base_ai import BaseAI

class QLearningAI(BaseAI):
    def __init__(self, lr=0.1, gamma=0.9, epsilon=0.1):
        self.lr = lr
        self.gamma = gamma
        self.epsilon = epsilon
        self.Q = {}  # Q[(state)][action]

    def encode_state(self, paddle, ball):
        zone = int((ball.y - paddle.y) // 30)
        direction = 1 if ball.vy > 0 else -1
        speed = int(abs(ball.vx) // 2)
        return (zone, direction, speed)

    def get_actions(self):
        return [0, 1, 2]

    def decide(self, paddle, ball, arena):
        state = self.encode_state(paddle, ball)

        # explorace vs. exploatace
        if random.random() < self.epsilon:
            action = random.choice(self.get_actions())
        else:
            Qs = self.Q.get(state, {a: 0 for a in self.get_actions()})
            action = max(Qs, key=Qs.get)

        return {
            "up": action == 1,
            "down": action == 2
        }

    def give_reward(self, paddle, ball, reward):
        state = self.encode_state(paddle, ball)
        if state not in self.Q:
            self.Q[state] = {a: 0 for a in self.get_actions()}

        best_next = max(self.Q[state].values())
        self.Q[state][self.last_action] += self.lr * (reward + self.gamma * best_next)
```

---

# ğŸ§¬ 8. LEVEL 4 â€“ NeuronovÃ¡ sÃ­Å¥ (TensorFlow / PyTorch)

MoÅ¾nosti:

* predikce mÃ­sta dopadu
* vÃ½bÄ›r akce (nahoru/dolÅ¯/nic)
* trÃ©nink na zÃ¡znamech ze hry

UkÃ¡zka mini-modelu (TensorFlow):

`soubor: ai/nn_ai.py`

```python
import tensorflow as tf
from .base_ai import BaseAI

class NeuralAI(BaseAI):
    def __init__(self, model_path="model.h5"):
        self.model = tf.keras.models.load_model(model_path)

    def decide(self, paddle, ball, arena):
        features = [
            ball.x, ball.y,
            paddle.x, paddle.y,
            ball.vx, ball.vy
        ]
        inputs = tf.convert_to_tensor([features], dtype=tf.float32)
        out = self.model(inputs)[0]

        # tÅ™i hodnoty pro tÅ™i moÅ¾nÃ© akce
        up, stay, down = out.numpy()

        return {
            "up": up > stay and up > down,
            "down": down > stay and down > up
        }
```

---

# ğŸ§© 9. Integrace AI do `MultipongEngine`

PÅ™idÃ¡me moÅ¾nost, aby pÃ¡lka mÄ›la pÅ™idÄ›lenou AI:

```python
paddle.ai = SimpleAI()
```

NÃ¡slednÄ› v `MultipongEngine.update()`:

```python
if paddle.ai is not None:
    action = paddle.ai.decide(paddle, self.ball, self.arena)
    up = action["up"]
    down = action["down"]
else:
    # hrÃ¡ÄskÃ© vstupy
    p_id = paddle.stats.player_id
    up = inputs.get(p_id, {}).get("up", False)
    down = inputs.get(p_id, {}).get("down", False)
```

Tento mechanismus umoÅ¾Åˆuje:

* kombinaci lidskÃ½ch i AI hrÃ¡ÄÅ¯
* hru 1v4, 4v1, 4v4, 2v3 â€¦

---

# ğŸ® 10. UI pro volbu AI ÃºrovnÄ›

V menu (pozdÄ›ji):

* vybrat poÄet AI hrÃ¡ÄÅ¯
* u kaÅ¾dÃ©ho napsat:

  * `Human`
  * `AI Simple`
  * `AI Predictive`
  * `AI Q-Learning`
  * `AI Neural`

StandardnÄ›:

* tÃ½m B mÅ¯Å¾e mÃ­t vÅ¾dy alespoÅˆ 1 AI pro PvE reÅ¾im
* pokud se ÄlovÄ›k pÅ™ipojÃ­ pÅ™es WebSocket, AI se vypne

---

# ğŸ§ª 11. Mini Ãºkoly pro studenty

### ğŸ”¹ 1) UdÄ›lej prediktivnÃ­ AI mÃ©nÄ› dokonalou

PÅ™idej nÃ¡hodnÃ½ Å¡um:
`target_y += random.uniform(-20, 20)`

### ğŸ”¹ 2) Uprav Q-learning tak, aby mÄ›l â€Ãºnavuâ€œ

ÄŒÃ­m dÃ©le se hra hraje, tÃ­m vÃ­ce chyb dÄ›lÃ¡.

### ğŸ”¹ 3) PÅ™idej logovÃ¡nÃ­ chovÃ¡nÃ­ AI

VypisovÃ¡nÃ­ toho, proÄ se AI rozhodla tak Äi onak.

### ğŸ”¹ 4) Copilot prompt

> â€VytvoÅ™ datovou strukturu pro uklÃ¡dÃ¡nÃ­ zkuÅ¡enostÃ­ RL agenta (experience replay).â€œ


