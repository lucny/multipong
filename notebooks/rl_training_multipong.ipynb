{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4aa21de",
   "metadata": {},
   "source": [
    "## 1. Importy a p≈ô√≠prava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from multipong.ai import RLPongEnv, encode_state, update_q_value, get_q_value\n",
    "\n",
    "# Nastaven√≠ seed pro reprodukovatelnost\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe25990",
   "metadata": {},
   "source": [
    "## 2. Inicializace prost≈ôed√≠ a hyperparametr≈Ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d69eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prost≈ôed√≠\n",
    "env = RLPongEnv(width=400, height=300, paddle_height=60, ball_speed=4)\n",
    "\n",
    "# Hyperparametry Q-learningu\n",
    "episodes = 2000\n",
    "alpha = 0.15      # Learning rate\n",
    "gamma = 0.95      # Discount factor\n",
    "epsilon = 0.2     # Exploration rate\n",
    "decay = 0.995     # Epsilon decay (pokud chce≈° sni≈æovat exploraci)\n",
    "num_bins = 8      # Poƒçet bin≈Ø pro diskretizaci\n",
    "\n",
    "# Akce\n",
    "ACTIONS = [0, 1, 2]  # 0=stay, 1=up, 2=down\n",
    "\n",
    "# Q-tabulka\n",
    "Q = {}\n",
    "\n",
    "print(f\"üéÆ Tr√©nink RL agenta na RLPongEnv\")\n",
    "print(f\"üìä Hyperparametry: Œ±={alpha}, Œ≥={gamma}, Œµ={epsilon}\")\n",
    "print(f\"üîÅ Epizod: {episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00dfba",
   "metadata": {},
   "source": [
    "## 3. Tr√©ninkov√° smyƒçka Q-learningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_episode = []\n",
    "current_epsilon = epsilon\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    state_key = encode_state(state, env, num_bins=num_bins)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    done = False\n",
    "    while not done and steps < 500:  # max 500 krok≈Ø na epizodu\n",
    "        # Epsilon-greedy: explorace vs. exploatace\n",
    "        if random.random() < current_epsilon:\n",
    "            action_idx = random.randint(0, len(ACTIONS) - 1)\n",
    "        else:\n",
    "            q_vals = get_q_value(Q, state_key)\n",
    "            action_idx = int(np.argmax(q_vals))\n",
    "\n",
    "        action = ACTIONS[action_idx]\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        next_state_key = encode_state(next_state, env, num_bins=num_bins)\n",
    "\n",
    "        # Q-learning update\n",
    "        update_q_value(Q, state_key, action_idx, next_state_key, reward, alpha=alpha, gamma=gamma)\n",
    "\n",
    "        state_key = next_state_key\n",
    "\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    current_epsilon *= decay  # Sni≈æuj exploraci\n",
    "\n",
    "    # Progress report\n",
    "    if (ep + 1) % 200 == 0:\n",
    "        avg_reward = np.mean(rewards_per_episode[-100:])\n",
    "        print(f\"Ep {ep+1:4d}/{episodes} | Avg reward (100): {avg_reward:7.2f} | Œµ: {current_epsilon:.4f}\")\n",
    "\n",
    "print(f\"‚úÖ Tr√©nink dokonƒçen! Q-tabulka m√° {len(Q)} stav≈Ø.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d5a39",
   "metadata": {},
   "source": [
    "## 4. Anal√Ωza v√Ωsledk≈Ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deceb8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistika\n",
    "print(f\"\\nüìà Statistika tr√©ninku:\")\n",
    "print(f\"  Poƒçet stav≈Ø v Q-tabulce: {len(Q)}\")\n",
    "print(f\"  Poƒçet akc√≠: {len(ACTIONS)}\")\n",
    "print(f\"  Pr≈Ømƒõrn√° odmƒõna (prvn√≠ch 100): {np.mean(rewards_per_episode[:100]):.2f}\")\n",
    "print(f\"  Pr≈Ømƒõrn√° odmƒõna (posledn√≠ch 100): {np.mean(rewards_per_episode[-100:]):.2f}\")\n",
    "print(f\"  Min. odmƒõna: {np.min(rewards_per_episode):.2f}\")\n",
    "print(f\"  Max. odmƒõna: {np.max(rewards_per_episode):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdac544",
   "metadata": {},
   "source": [
    "## 5. Vizualizace k≈ôivky uƒçen√≠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Vyhlazen√° k≈ôivka (klouzav√Ω pr≈Ømƒõr)\n",
    "    window = 50\n",
    "    smoothed = [\n",
    "        np.mean(rewards_per_episode[max(0, i - window) : i + 1])\n",
    "        for i in range(len(rewards_per_episode))\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Graf 1: Surov√© a vyhlazen√© odmƒõny\n",
    "    axes[0].plot(rewards_per_episode, alpha=0.3, label=\"Surov√© odmƒõny\")\n",
    "    axes[0].plot(smoothed, linewidth=2, label=\"Vyhlazeno (klouzav√Ω pr≈Ømƒõr)\")\n",
    "    axes[0].set_xlabel(\"Epizoda\")\n",
    "    axes[0].set_ylabel(\"Celkov√° odmƒõna za epizodu\")\n",
    "    axes[0].set_title(\"Kr√°tka uƒçen√≠ RL agenta\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Graf 2: Pr≈Ømƒõr posledn√≠ch N epizod\n",
    "    window2 = 100\n",
    "    running_avg = [\n",
    "        np.mean(rewards_per_episode[max(0, i - window2) : i + 1])\n",
    "        for i in range(len(rewards_per_episode))\n",
    "    ]\n",
    "    axes[1].plot(running_avg, linewidth=2, color=\"green\")\n",
    "    axes[1].set_xlabel(\"Epizoda\")\n",
    "    axes[1].set_ylabel(\"Pr≈Ømƒõrn√° odmƒõna (100 epizod)\")\n",
    "    axes[1].set_title(\"Konvergence - pr≈Ømƒõr posledn√≠ch 100 epizod\")\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Grafy vykresleny\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è matplotlib nen√≠ nainstalov√°n, grafy se nevykresluji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05be356",
   "metadata": {},
   "source": [
    "## 6. Evaluace natr√©novan√©ho agenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluace bez explorace (pure exploitation)\n",
    "eval_episodes = 100\n",
    "eval_rewards = []\n",
    "\n",
    "for ep in range(eval_episodes):\n",
    "    state = env.reset()\n",
    "    state_key = encode_state(state, env, num_bins=num_bins)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < 500:\n",
    "        # V≈ædy zvol√≠me best action (greedy)\n",
    "        q_vals = get_q_value(Q, state_key)\n",
    "        action_idx = int(np.argmax(q_vals))\n",
    "        action = ACTIONS[action_idx]\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        state_key = encode_state(next_state, env, num_bins=num_bins)\n",
    "\n",
    "    eval_rewards.append(total_reward)\n",
    "\n",
    "print(f\"\\nüéØ EVALUACE (bez explorace):\")\n",
    "print(f\"  Epizod: {eval_episodes}\")\n",
    "print(f\"  Pr≈Ømƒõrn√° odmƒõna: {np.mean(eval_rewards):.2f}\")\n",
    "print(f\"  Std. dev: {np.std(eval_rewards):.2f}\")\n",
    "print(f\"  Min: {np.min(eval_rewards):.2f}\")\n",
    "print(f\"  Max: {np.max(eval_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b06f03",
   "metadata": {},
   "source": [
    "## 7. Ulo≈æen√≠ modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2341009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvo≈ô slo≈æku pro modely\n",
    "models_dir = Path(\"multipong/ai/models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = models_dir / \"q_table_pong.pkl\"\n",
    "\n",
    "# Ulo≈æ Q-tabulku\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(Q, f)\n",
    "\n",
    "print(f\"‚úÖ Model ulo≈æen: {model_path}\")\n",
    "print(f\"   Velikost: {model_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d4e672",
   "metadata": {},
   "source": [
    "## 8. Naƒçten√≠ a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vyzkou≈°ej naƒçten√≠ modelu\n",
    "with open(model_path, \"rb\") as f:\n",
    "    Q_loaded = pickle.load(f)\n",
    "\n",
    "print(f\"‚úÖ Model naƒçten zpƒõt!\")\n",
    "print(f\"   Stav≈Ø v Q-tabulce: {len(Q_loaded)}\")\n",
    "print(f\"   Kontrola: Q-tabulka je identick√°: {Q == Q_loaded}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
